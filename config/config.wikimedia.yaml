# Example config file for running a wikimedia-eventgate instance.
# This (optionally) uses a stream config file to restrict schemas
# in streams.

# Number of worker processes to spawn.
# Set to 0 to run everything in a single process without clustering.
# Use 'ncpu' to run as many workers as there are CPU units
num_workers: 0

# Log error messages and gracefully restart a worker if v8 reports that it
# uses more heap (note: not RSS) than this many mb.
worker_heap_limit_mb: 250

# Logger info
logging:
  level: debug

#  streams:
#  # Use gelf-stream -> logstash
#  - type: gelf
#    host: logstash1003.eqiad.wmnet
#    port: 12201

# Statsd metrics reporter
# metrics:
#   name: eventgate
#   type: statsd
#   host: 0.0.0.0
#   port: 8125

services:
  - name: EventGate
    # a relative path or the name of an npm package, if different from name
    module: ./app.js
    # optionally, a version constraint of the npm package
    # version: ^0.4.0
    # per-service config
    conf:
      port: 8192
      # uncomment to only listen on localhost
      #interface: localhost

      # Set cors to false by default.  For usage see:
      # https://github.com/wikimedia/service-template-node/blob/1784fd19fcab699cda44b429ea31fa47e5e82793/config.prod.yaml#L39
      cors: false

      # more per-service config settings
      user_agent: eventgate

      eventgate_factory_module: '../lib/factories/wikimedia-eventgate'

      # Mapping of stream names to allowed schemas
      stream_config_uri: ./config/wikimedia-stream-config.yaml

      # This field in each event will be used to extract a
      # (possibly relative) schema uri.  The default is $schema.
      # An array of field names will cause EventGate to search for
      # You can alteratively use a remote HTTP base URI.
      # fields by these names in each event, using the first match.
      schema_uri_field: [$schema, meta.schema_uri]

      # If set, these URIs will be prepended to any relative schema URI
      # extracted from each event's schema_field.  The resulting URLs will
      # be searched until a schema is found.  Change this
      # to match paths to your schema repositories.
      schema_base_uris: [../event-schemas/jsonschema/]
      # You can alteratively or additionally use a remote HTTP base URI.
      #schema_base_uris: [https://raw.githubusercontent.com/wikimedia/mediawiki-event-schemas/master/jsonschema]

      # These schema URIs will be 'precached' on service startup.
      # They should be resolveable by the URI prefixes in schema_base_uris.
      # TODO: we could get this list from a service?
      schema_precache_uris:
        - /error/0.0.3
        - /test/event/0.0.2
        - /mediawiki/api/request/0.0.1

      # If set, this will be appended to every extracted schema_uri if that schema_uri
      # does not already end with a file extension.
      #schema_file_extension: .yaml

      # If this is true, then schema_uris in events will be allowed to
      # point at any URL domain.  If false, they must only contain URI paths,
      # which will be prefixed with one of the schema_base_uris.
      # Setting this to false restricts schema URLs to those specified
      # in schema_base_uris.  This defaults to false.
      allow_absolute_schema_uris: false

      # This field in each event will be used to extract a destination 'stream' name.
      # This will equal the destination Kafka topic, unless a topic prefix
      # is also configured.
      stream_field: [meta.stream, meta.topic]
      topic_prefix: 'datacenter1.'

      # This field will be used in log messages to uniquely ID each event.
      id_field: meta.id

      # This field will be used to extract and set a Kafka message timestamp.
      dt_field: meta.dt

      # If a validation error is encountered, a validation error event
      # will be produced to this stream.
      error_stream: eventgate.error.validation

      # kafka configs go here.
      kafka:
        conf:
          metadata.broker.list: 'localhost:9092'
          compression.codec: snappy
          message.max.bytes: 4194304
          # report Kafka producer stats every 5 seconds
          statistics.interval.ms: 5000
          ## Uncomment the below to enable rdkafka trace logging
          # event_cb: true
          # log_level: 7
          # debug: broker,topic,msg
        # kafka topic conf goes here
        topic_conf: {}

        # Producer type specific overrides.
        # If you need to configure some producer specific settings,
        # e.g. different batch settings, you can provide them here.
        hasty:
          conf:
            # HastyProducer doesn't block HTTP clients, so we can
            # afford to wait for a largish batch size.
            queue.buffering.max.ms: 1000
        guaranteed:
          conf:
            # GuaranteedProducer doese block HTTP clients, so we
            # should use a smaller batch size. Waiting 50 ms
            # will allow for some kafka produce reqeust batching
            # while minimizing latency on the HTTP client.
            queue.buffering.max.ms: 50
